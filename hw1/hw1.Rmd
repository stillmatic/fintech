---
title: "Credit Modelling"
author:
- Chris Hua
date: "11/7/2016"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.height = 5, fig.width = 8)
library(dplyr)
library(knitr)
library(magrittr)
library(broom)
library(ggplot2)
library(pROC)
library(tidyr)
insample <- read.csv("File1_IS_data.csv")
```

# 0: Data Exploration

First, we note the distribution of the outcome variable:

```{r, comment=""}
insample %>% 
    select(default) %>% table()
```

As well as the distribution of the FICO score, among applicants in this dataset. This is a pretty heavily left-skewed distribution, which probably represents the fact that the people who try to get loans on Lending Club are people who can't get loans traditionally.

```{r}
insample %>%
    ggplot(aes(fico)) + geom_histogram(bins = 35) + 
    theme_classic() + 
    ggtitle("") + xlab("FICO Score")
```

# 1: Basic Modelling

We model "default probability" with respect to "FICO" score only. Here, a positive outcome denotes that the user defaulted, and a negative outcome denotes that the user did not default. Note that we have to relabel the factor levels for this to work by default.

**a.** We expect a negative coefficient on FICO. Intuitively, a customer is less likely to default the better their credit score is. We would also expect a positive intercept, because for a customer with 0 credit score (the condition for the intercept), they have terrible chances of paying off the bill, and thus high chance for default.

**b.** We estimate our model using a generalized linear model (`glm`) approach, with a binomial prior. Summary statistics of the model follow:

```{r}
insample %<>%
   mutate(default = default == "Defaulted")

log_fit <- glm(default ~ fico, data = insample, family = "binomial")
log_fit %>% tidy %>% knitr::kable(digits = 2)
```

By p-value, `fico` is a statistically significant value at better than at 0.01 level, indicating that by itself, `fico` is a useful predictor for default outcomes.

The intercept is strongly positive, which indicates that default is very likely for somebody with a `fico` score of 0. Alternatively, the intercept suggests that somebody with a score of 768 has 0 chance of default. The negative coefficient means that for each point of FICO score, the likelihood of default decreases by 0.01. Each of these observations lines up with our intuitions.

# 2: Model Evaluation

**(a)** We can estimate a probability of default via the formula from class 4:

$$Pr(f) = \frac{\exp(\beta_0 -\beta_1 \times f)}{1 + \exp(\beta_0 - \beta_1 \times f)}$$

Since our model only includes FICO scores, we can easily plot FICO vs estimated probability of default.

```{r}
# predict in base R does not play nice with pipes
log_odds <- function(model, val) {
    betas <- model %>% tidy %$% estimate
    b_0 <- betas[1]
    b_1 <- betas[2]
    x = exp(b_0 + (b_1 * val))
    x / (1 + x)
}

insample %>%
    mutate(prob = log_odds(log_fit, fico)) %>%
    ggplot(aes(x = fico, y = prob)) + geom_line() +
    ggtitle("Estimated probability of default") + 
    xlab("FICO") + ylab("Estimated %") + 
    theme_bw()
```

Interestingly, we can compare this plot to the empirically determined default risk. We can see that the estimated curve fits very well, except for the outliers in high credit score. This might tell us that high credit score borrowers have asymmetric information about their ability to pay off loans, and they are the 'lemons' in this market.

```{r}
insample %>%
    group_by(fico) %>%
    summarize(empirical = (mean(as.numeric(default)))) %>%
    mutate(estimated = log_odds(log_fit, fico)) %>%
    gather(key = "est_type", value = "risk",
           ... = empirical, estimated) %>% 
    ggplot(aes(x = fico, y = risk, color = est_type)) + 
    geom_point() + theme_bw() + 
    theme(legend.position = "bottom") + 
    xlab("FICO") + ylab("") + 
    ggtitle("Empirical vs estimated default risk")
```


**(b)** Then, we can plot a corresponding ROC curve for this model:

```{r, message=F, fig.width=5, fig.height=5}
log_roc <- insample %>%
    mutate(prob = log_odds(log_fit, fico)) %>%
    roc(default ~ prob, data = .) %>%
    suppressMessages()

plot_log_roc <- log_roc %>% plot
plot_log_roc
```

**(c)** For this model, we have AUC of `r round(log_roc$auc, 3)`. This is better than 0.5. It's consistent with the findings in part 1b, where we found that FICO was a statistically significant predictor alone of default.

**(d)** This is essentially using a 0.1 threshold on our probabilities of default. Then, we can create a confusion matrix:

```{r}
low_thresh_vals <- insample %>% 
    select(fico) %>%
    log_odds(log_fit, .) %>%
    is_greater_than(0.1)

confusion <- table(low_thresh_vals, insample$default)
confusion %>% knitr::kable()
```

The proportion correctly rejected is `r round(confusion[1,1] / sum(confusion[,1]), 3)`. The proportion mistakenly rejected is `r round(1 - (confusion[1,1] / sum(confusion[,1])), 3)`. Whether this is acceptable or not is dependent on the cost of a false positive vs false negative for the lender.

# 3: An out-of-sample analysis

Note that we use the first 9000 rows as training and last 1000 rows as test - in protest. It would be much more theoretically legitimate to randomly sample the rows to use.

```{r}
data_small <- insample[1:9000,]
data_out <- insample[9001:10000,]

# this is a bad idea...should randomly sample instead
# set.seed(8)
# data_small <- insample %>% sample_n(9000, replace= F)
```

**(a)** The new model, estimated on the first 9000 rows, is given by these summary statistics:

```{r}
small_fit <- glm(default ~ fico, data = data_small, family = "binomial")

small_fit %>% tidy %>% knitr::kable(digits = 2)
```

This is pretty similar to the model that we estimated on the full 1000 data points, but has a less negative intercept.

**(b)** Then, we can predict probabilities for the remaining loans, and then create an ROC curve for both fits:

```{r, message=F}
plot(log_roc)
data_out %>%
    mutate(pred = log_odds(small_fit, fico)) %>%
    roc(default ~ pred, data = ., ) %>%
    plot.roc(add = T)
``` 

**(c)** The area below the new ROC curve gets larger. My inclination is that we are no longer overfitting our dataset.

**(d)** Vacuously, you don't want to use all variables available, because some of the variables are unique to a person or a loan- e.g. "id". These could perfectly estimate somebody's probability of default in the sample set but have no predictive use.

If we fit every variable into the logistic model, we run the chance of overfitting. This is where we find some spurious behavior in the training data, that isn't representative of the underlying data, but rather an artifact of the existing data points.

Finally, we also may want parsimoniousness in our model, which is having fewer variables and being able to explain the data with these fewer variables. It is harder to explain a 10 variable model than a 3 variable model.

**e** Let's consider interest rate, employment length, and annual income. We're going to do this on test-train split as well. We make one adjustment to the data - we divide annual income by 1000. Ordinary least squares regression is scale-invariant 

We fit the model and show the Anova table (type II tests):

```{r}
data_out %<>%
    mutate(annual_inc = annual_inc / 1000)
fit_larger <- glm(default ~ fico + int_rate + emp_length + annual_inc, data = data_out, family = "binomial")
car::Anova(fit_larger)
```

Within this model, each variable is significant at the 0.05 level except the length of employment. We can kick out that variable to create a more parsimonious, 3 variable, model. We will use this as our final model.

```{r}
fit_larger2 <- glm(default ~ fico + int_rate + annual_inc, data = data_out, family = "binomial")
car::Anova(fit_larger2)
```

```{r}
fit_acc_temp <- fit_larger2 %>% 
    predict(data = data_out, type = "response") %>%
    cbind(data_out)
names(fit_acc_temp)[1] <- "pred"
```

We can test the accuracy of this model in a similar way as before. We add the plot of the new model's ROC curve to the existing model's ROC curve plot, and compare them. The new curve is chunkier because it is comparing 1000 points vs 9000. However, we see that this is a better fit, with ROC of `r fit_acc_temp %>% roc(default ~ pred, data = .) %>% as.list %>% magrittr::extract("auc")`

```{r}
data_out %>%
    mutate(pred = log_odds(small_fit, fico)) %>%
    roc(default ~ pred, data = ., ) %>% plot
fit_acc_temp %>%
    roc(default ~ pred, data = .) %>% plot(add = T)
```

Thus, our final model is given by these summary statistics. Each variable is significant, and each has a clear interpretation.

```{r}
fit_larger2 %>% tidy %>% knitr::kable(digits = 4)
```


# 4: A business decision to make

```{r}
outsample <- read.csv("File2_OOS_predictor_data.csv")
```

```{r}
# get Pr(default | X)
outsample %<>% mutate(annual_inc = annual_inc / 1000)
outsample$default_prob <-  fit_larger2 %>% predict(newdata = outsample, type="response")
```

Our expected value is, for any given loan $i$,:

$$EV_{i} = \frac{1 - (1+r_i)^{-n}}{r_i} \times \Pr(payoff \mid X_i) \times A_{i}$$

Then, $\frac{1 - (1+r_i)^{-n}}{r_i}$ is a standard present value of annuity formula, where $r_i$ is the interest rate we charge and $n$ is the number of years that the loan will run. Finally, $\Pr(payoff \mid X_i)$ is the probability that the user will pay off the loan, given some characteristics $X_i$ at the user level, and $A_i$ is the amount of the loan. Note that the payoff probability comes from the model which we estimate in part 3. Additionally, note that we assume $n=5$ for Lending Club loans.

This is a reasonable function because we care about the interest to be earned over the life of the loan, the chance of default, and the total amount of the loan. This is simplified somewhat because we have both liquidity constraints and investment constraints - in the absence of investment constraints we would allocate some money to this risky loan portfolio but also to a risk-free or market portfolio, and in the absence of liquidity constraints we would back each loan with a positive expected value. We additionally assume that loans are either fully paid off or fully written off, which isn't a reasonable assumption in the real world, since we can usually expect to recoup some of the value of a failed loan.

```{r}
portfolio <- outsample %>%
    mutate(int_multiple = ( (1 - (int_rate / 100))^-5) / int_rate) %>%
    mutate(e_return = default_prob * int_multiple * loan_amnt) %>%
    select(id, e_return, loan_amnt, default_prob, int_multiple) %>%
    arrange(-e_return) %>%
    head(100)

csv_out <- outsample %>%
    transmute(id = id, picked = id %in% portfolio$id)

csv_out %>% write.csv(file = "applicant_portfolio.csv")
```

On these 100 loans, we expect a `r round((sum(portfolio$loan_amnt) + sum(portfolio$e_return)) / sum(portfolio$loan_amnt) - 1, 2)*100`% return.

One improvement we could make to our model is to add a prior distribution on when loans are likely to default. We might expect the curve to be concave down, increasing then decreasing. This can give us a better idea of when the loans are likely to default, and then give us a better idea of how much we can actually earn from them. But: we don't have any data about that, and since our goal is to rank rather than predict the actual value, we should be fine wits this model.
