---
title: "Credit Modelling"
author:
- Chris Hua
- Kevin Huo
- Arjun Jain
- Juan Manubens
date: "11/7/2016"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.height = 5, fig.width = 8)
library(dplyr)
library(knitr)
library(magrittr)
library(broom)
library(ggplot2)
library(pROC)
library(tidyr)
insample <- read.csv("File1_IS_data.csv")
```

forgive me

# 1: Basic Modelling

We model "default probability" with respect to "FICO" score only. First, we note the distribution of the outcome variable:

```{r, comment=""}
insample %>% 
    select(default) %>% table()
```

As well as the distribution of the FICO score, among applicants in this dataset. This is a pretty heavily left-skewed distribution, which probably represents the fact that the people who try to get loans on Lending Club are people who can't get loans traditionally.

```{r}
insample %>%
    ggplot(aes(fico)) + geom_histogram(bins = 35) + 
    theme_classic() + 
    ggtitle("") + xlab("FICO Score")
```

**a.** We expect a negative coefficient on FICO. Intuitively, a customer is less likely to default the better their credit score is. We would also expect a positive intercept, because for a customer with 0 credit score (the condition for the intercept), they have terrible chances of paying off the bill, and thus high chance for default.

**b.** We estimate our model using `glm` approach with a binomial prior. Summary statistics:

```{r}
#insample %<>%
 #   mutate(default = default == "Defaulted")

log_fit <- glm(default ~ fico, data = insample, family = "binomial")
log_fit %>% tidy %>% knitr::kable(digits = 2)
```

By p-value, `fico` is a statistically significant value at better than at 0.01 level, indicating that by itself, `fico` is a useful predictor for default outcomes.

The intercept is strongly positive, which indicates that default is very likely for somebody with a `fico` score of 0. Alternatively, the intercept suggests that somebody with a score of 768 has 0 chance of default. The positive coefficient means that for each point of FICO score, the likelihood of a negative outcome decreases by 0.01. Each of these observations lines up with our intuitions.

# 2: Model Evaluation

**(a)** We can estimate a probability of default via the formula from class 4:

$$Pr(f) = \frac{\exp(\beta_0 -\beta_1 \times f)}{1 + \exp(\beta_0 - \beta_1 \times f)}$$

Since our model only includes FICO scores, we can easily plot FICO vs estimated probability of default.

```{r}
prob_default <- function(fico) {
    # empirically estimated beta's from above
    b_0 = 7.68107
    b_1 = -0.01341
    x = exp(b_0 + (b_1 * fico))
    x / (1 + x)
}
insample %>%
    mutate(prob = prob_default(fico)) %>%
    ggplot(aes(x = fico, y = prob)) + geom_line() +
    ggtitle("Estimated probability of default") + 
    xlab("FICO") + ylab("Estimated %") + 
    theme_bw()
```

Interestingly, we can compare this plot to the empirically determined default risk. We can see that the estimated curve fits very well, except for the outliers in high credit score. This might tell us that high credit score borrowers have asymmetric information about their ability to pay off loans, and they are the 'lemons' in this market.

```{r}
insample %>%
    group_by(fico) %>%
    summarize(empirical = 1- (mean(as.numeric(default)) - 1)) %>%
    mutate(estimated = prob_default(fico)) %>%
    gather(key = "est_type", value = "risk",
           ... = empirical, estimated) %>% 
    ggplot(aes(x = fico, y = risk, color = est_type)) + 
    geom_point() + theme_bw() + 
    theme(legend.position = "bottom") + 
    xlab("FICO") + ylab("") + 
    ggtitle("Empirical vs estimated default risk")
```


**(b)** Then, we can plot a corresponding ROC curve for this model:

```{r, message=F, fig.width=5, fig.height=5}
log_roc <- insample %>%
    mutate(prob = prob_default(fico)) %>%
    roc(default ~ prob, data = .) %>%
    suppressMessages()

log_roc %>% plot
```

**(c)** For this model, we have AUC of `r log_roc$auc`. This is better than 0.5. It's consistent with the findings in part 1b, where we found that FICO was a statistically significant predictor alone of default.

**(d)** This is essentially using a 0.1 threshold on our probabilities of default. Then, we can create a confusion matrix:

```{r}
low_thresh_vals <- insample %>% 
    select(fico) %>%
    prob_default %>%
    is_greater_than(0.1)

confusion <- table(low_thresh_vals, insample$default)
confusion %>% knitr::kable()
```

The proportion correctly rejected is `r confusion[1,1] / sum(confusion[,1])`. The proportion mistakenly rejected is `r 1 - (confusion[1,1] / sum(confusion[,1]))`. This is no bueno - our threshold is probably too low.
# 3: An out-of-sample analysis

```{r}
data_small <- insample[1:9000,]
data_out <- insample[9001:10000,]

# this is a bad idea...should randomly sample instead
# set.seed(8)
# data_small <- insample %>% sample_n(9000, replace= F)
```

**(a)** The new model, estimated on the first 9000 rows, is given by these summary statistics:

```{r}
small_fit <- glm(default ~ fico, data = data_small, family = "binomial")

small_fit %>% tidy %>% knitr::kable(digits = 2)
```

**(b)** Then, we can predict probabilities for the remaining loans, and then create an ROC curve for both fits:

```{r, message=F}
prob_default_small <- function(fico) {
    b_0 = 7.16430865
    b_1 = -0.01268825
    x = exp(b_0 + (b_1 * fico))
    x / (1 + x)
}
plot(log_roc)
data_out %>%
    mutate(pred = prob_default_small(fico)) %>%
    roc(default ~ pred, data = ., ) %>%
    plot.roc(add = T)
``` 

**(c)** The area below the new ROC curve gets larger. We are no longer overfitting our dataset.

**(d)** Vacuously, you don't want to use all variables available, because some of the variables are unique to a person or a loan- e.g. "id". These could perfectly estimate somebody's probability of default in the sample set but have no predictive use.

[overfit...]

Finally, we also may want parsimoniousness [...]

**e** Let's consider interest rate, loan length, and annual income. We're going to do this on test-train split as well.

We fit the model and show the Anova table (type II tests):

```{r}
fit_larger <- glm(default ~ fico + int_rate + emp_length + annual_inc, data = data_out, family = "binomial")
car::Anova(fit_larger)
```

Within this model, each variable is significant at the 0.05 level except the length of the loan. We can kick out that variable to create a more parsimonious, 3 variable, model.

```{r}
fit_larger2 <- glm(default ~ fico + int_rate + annual_inc, data = data_out, family = "binomial")
car::Anova(fit_larger2)
```

We can test the accuracy of this model

```{r}
fit_acc_temp <- fit_larger2 %>% 
    predict(data = data_out, type = "response") %>%
    cbind(data_out)
names(fit_acc_temp)[1] <- "pred"
fit_acc_temp %>%
    roc(default ~ pred, data = .)
```

# 4: A business decision to make

```{r}
outsample <- read.csv("File2_OOS_predictor_data.csv")
```

```{r}
# recode into numeric time length
levels(outsample$emp_length) <- c("0.5", "1", "10", "2", "3", "4", "5", "6", "7", "8", "9")
outsample$emp_length <- as.numeric(paste(outsample$emp_length))

# get Pr(default | X)
outsample$default_prob <- fit_larger2 %>% predict(newdata = outsample, type="response")
```

Since we must pick 100 loans to finance, we need to construct a benchmark return on a low risk loan judging from the available options. 

```{r}
#(table(outsample$int_rate)) %>% cumsum %>% head
```

Our expected value is, for any given loan $i$,:

$$EV_{i} = \frac{1 - (1+r_i)^{-n}}{r_i} \times \Pr(payoff \mid X_i) \times A_{i}$$

Then, $\frac{1 - (1+r_i)^{-n}}{r_i}$ is a standard present value of annuity formula, where $r_i$ is the interest rate we charge and $n$ is the number of years that the loan will run. Finally, $\Pr(payoff \mid X_i)$ is the probability that the user will pay off the loan, given some characteristics $X_i$ at the user level, and $A_i$ is the amount of the loan. Note that the payoff probability comes from the model which we estimate in part 3.

This is a reasonable function because we care about the interest to be earned over the life of the loan, the chance of default, and the total amount of the loan. This is simplified somewhat because we have both liquidity constraints and investment constraints - in the absence of investment constraints we would allocate some money to this risky loan portfolio but also to a risk-free or market portfolio, and in the absence of liquidity constraints we would back each loan with a positive expected value. We additionally assume that loans are either fully paid off or fully written off, which isn't a reasonable assumption in the real world, since we can usually expect to recoup some of the value of a failed loan.

```{r}
portfolio <- outsample %>%
    filter(emp_length > 0) %>%
    mutate(int_multiple = ( (1 - (int_rate / 100) + 1)^-emp_length) / int_rate) %>%
    mutate(e_return = default_prob * int_multiple * loan_amnt) %>%
    select(id, e_return, loan_amnt, default_prob, int_multiple) %>%
    arrange(-e_return) %>%
    head(100)

csv_out <- outsample %>%
    transmute(id = id, picked = id %in% portfolio$id)
```

On these 100 loans, we expect a `r (sum(portfolio$loan_amnt) + sum(portfolio$e_return)) / sum(portfolio$loan_amnt) - 1` return. That sucks.


